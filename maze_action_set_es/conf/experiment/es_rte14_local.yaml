# @package _global_
# (this is the entry point for configuring your es training runs)

# list of default sub-configurations
defaults:
  - override /env: unitary
  - override /model: default
  - override /wrappers: default
  - override /reward: flat
  - override /algorithm: es
  - override /runner: local

# algorithm specific overrides
# (see https://maze-rl.readthedocs.io/en/latest/trainers/maze_trainers.html#evolutionary-strategies-es)
algorithm:
  # The number of epochs to train before termination. Pass 0 to train indefinitely
  n_epochs: 0
  # Minimum number of episode rollouts per training iteration (=epoch)
  n_rollouts_per_update: 1000
  # Limit the episode rollouts to a maximum number of steps. Set to 0 to disable this option.
  max_steps: 0

  # Support for simulation logic or heuristics on top of a TorchPolicy.
  policy_wrapper:
    # Action set based simulation policy as described in Zhou et al (2021)
    _target_: maze_action_set_es.agents.simulation_search_policy.SimulationSearchPolicy

    # Simulation environment used for action sampling
    simulated_env:
      _target_: maze.core.utils.config_utils.make_env
      env: ${env}
      wrappers: ${wrappers}

    # Number of action candidates to consider for simulation.
    top_k_candidates: 25

# standard training output directory
log_base_dir: outputs
